{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages'],\n",
      "        num_rows: 117\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load in dataset\n",
    "raw_dataset = datasets.load_from_disk(\"dataset.hf\")\n",
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I debrief during deliberations?\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = raw_dataset[\"train\"][0]\n",
    "example[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user                :  How do I debrief during deliberations?\n",
      "\n",
      "assistant           :  Deliberations can get pretty heated. It’s important to focus on the applicants and their many amazing qualities and not attack your subteam members. We all have the same goal: to accept the best candidates possible. No matter how heated the deliberations get, stay respectful and stay objective. \\n Afterwards, the team should debrief. It’s always good to get feedback on our recruitment process, both internally and externally. Recruitment is vital to the culture of CDS, so we need to make sure that we critically evaluate our internal processes each semester. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = example[\"messages\"]\n",
    "for message in messages:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    print('{0:20}:  {1}'.format(role, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "  tokenizer.model_max_length = 2048\n",
    "\n",
    "# Set chat template\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0584b1a12449a2a5163a97f800d41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=12):   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd28ed84e14ecfb41c8139aaee675f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=12):   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 45 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "What are the responsibilities of officers on CDS?\n",
      "</s>\n",
      "<|assistant|>\n",
      "a. Each officer, both elected and appointed, shall be responsible for their duties as listed in this constitution. \\n b. Each officer shall also be responsible for establishing and maintaining a working relationship with the General Membership, Executive Board, and all other officers, advisors, chairs, committees, and staff necessary to perform their duties.\n",
      "</s>\n",
      "\n",
      "Sample 39 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "How do I measure teamwork for written applications?\n",
      "</s>\n",
      "<|assistant|>\n",
      "0 - No mention of working on a team or with others. Applicant is very self-centered or does not consider teamwork. \\n 1 - Mentions working with others on some project. Calls out maintaining positive relationships or specifically how they resolved a conflict. Has experience working collaboratively with others, or mentoring others. \\n 2 - Working with others in detail, clear communication throughout writing. Focuses on teamwork more than leadership. Recognizes when and how to rely on others rather than take ownership. \n",
      "</s>\n",
      "\n",
      "Sample 62 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Can you give me some background about the interview process for CDS?\n",
      "</s>\n",
      "<|assistant|>\n",
      "For most of CDS’s history, each subteam has had a completely different interview. IntSys had the presentation, DE had the system design question, Insights had a data exploration question, etc. All of these interviews had one thing in common: they gave candidates a chance to prepare in advance. This is critical to our review - we frankly don’t care about candidates' ability to solve arbitrary LeetCode problems. It doesn’t help them when they get onto CDS and it’s a very shallow metric for potential on our team. \\n This semester, we’re going to try to standardize the interview processes across the subteams a bit more. One issue with all the different interviews is that the review is substantially different for each subteam. Again, it’s fine for subteams to make decisions that are best suited to their standards, but it should not be the case that the application process is substantially different depending on the subteam the candidate applies to. \n",
      "</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "column_names = list(raw_dataset[\"train\"].features)\n",
    "raw_datasets = raw_dataset.map(apply_chat_template,\n",
    "                                num_proc=cpu_count(),\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                                remove_columns=column_names,\n",
    "                                desc=\"Applying chat template\",)\n",
    "\n",
    "# create the splits\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=\"torch.bfloat16\",\n",
    "# )\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhan/miniconda3/envs/slackbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5844f5f6ed754c98a3d1e61c5b502442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc361ac2571f46afb0820d67234320dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1224387719924c63be92f4741a5912e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f2669272bc4b0da002d4828af46782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'model/zephyr-7b-sft-lora'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    # fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=128,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "    per_device_train_batch_size=1, # originally set to 8\n",
    "    # push_to_hub=True,\n",
    "    # hub_model_id=\"zephyr-7b-sft-lora\",\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model_id,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d5bf2fd9e86b5d2c8adef918d146cc04bc7de8e2b0aaa39d610905b5bf3bc85"
  },
  "kernelspec": {
   "display_name": "Python 3.10.13 ('slackbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
